{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv1gGmXE6-QJ",
        "outputId": "565f4a17-24c5-4f8f-aa08-af85c7972035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 1.0\n"
          ]
        }
      ],
      "source": [
        "## In scikit-learn, the bagging technique can be implemented using the `BaggingClassifier` or `BaggingRegressor` classes, depending on whether you're working on a classification or regression problem, respectively.\n",
        "\n",
        "## Here's an example code for using `BaggingClassifier` to train a bagged decision tree model:\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a decision tree classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create a bagging classifier using decision tree as base estimator\n",
        "bag_clf = BaggingClassifier(base_estimator=tree, n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
        "\n",
        "# Train the bagging classifier on the training data\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy score\n",
        "print(\"Accuracy score:\", accuracy)\n",
        "\n",
        "## In this code, we first load the iris dataset and split it into training and testing sets. We then create a `DecisionTreeClassifier` object as the base estimator for our bagging classifier. We set the number of estimators to 500 and the maximum number of samples to 100. We also enable bootstrapping and set the number of jobs to use for parallel processing to -1 to use all available processors.\n",
        "\n",
        "## We train the bagging classifier on the training data and make predictions on the testing data. Finally, we calculate the accuracy score of the model using the `accuracy_score` function from scikit-learn's `metrics` module and print it to the console."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting"
      ],
      "metadata": {
        "id": "RN8OWWfN7nAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In scikit-learn, the boosting technique can be implemented using the AdaBoostClassifier or AdaBoostRegressor classes, depending on whether you're working on a classification or regression problem, respectively.\n",
        "\n",
        "Here's an example code for using AdaBoostClassifier to train an boosted decision tree model:"
      ],
      "metadata": {
        "id": "AxddE6NJ7kDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a decision tree classifier\n",
        "tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "\n",
        "# Create an AdaBoost classifier using decision tree as base estimator\n",
        "ada_clf = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.5, random_state=42)\n",
        "\n",
        "# Train the AdaBoost classifier on the training data\n",
        "ada_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = ada_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy score\n",
        "print(\"Accuracy score:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWpw_bYs7lER",
        "outputId": "42b74bf6-b01e-4952-b14e-3ef66d10b240"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we first load the iris dataset and split it into training and testing sets. We then create a DecisionTreeClassifier object as the base estimator for our AdaBoost classifier. We set the number of estimators to 500 and the learning rate to 0.5.\n",
        "\n",
        "We train the AdaBoost classifier on the training data and make predictions on the testing data. Finally, we calculate the accuracy score of the model using the accuracy_score function from scikit-learn's metrics module and print it to the console.\n",
        "\n",
        "Note that we set max_depth=1 for the decision tree base estimator to make it a weak learner, which is necessary for boosting to work effectively. In practice, you may need to experiment with different values of hyperparameters such as n_estimators, learning_rate, and max_depth to optimize the performance of your model."
      ],
      "metadata": {
        "id": "ExaEpP2k7yUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging in simple terms\n",
        "\n",
        "Bagging (Bootstrap Aggregating) is a machine learning technique that involves training multiple models on different subsets of the training data to improve the accuracy and stability of the model. \n",
        "\n",
        "The idea behind bagging is to create multiple subsets of the training data by randomly sampling from the original dataset with replacement. Then, a model is trained on each of the subsets of data, and the results are combined to produce a final prediction. This approach helps to reduce overfitting by using a diverse set of models, which can improve the model's generalization performance on new data.\n",
        "\n",
        "For example, if we want to predict whether a customer will buy a product based on their age, gender, and income, we can use bagging to train multiple decision tree models on different subsets of the customer data. Each decision tree model might focus on different features of the customer data, such as age or income, and produce slightly different predictions. The final prediction would be an average or majority vote of the predictions from all the decision tree models, which can improve the accuracy and stability of the model."
      ],
      "metadata": {
        "id": "J3GkSUSS8Oky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting in simple terms\n",
        "\n",
        "Boosting is a machine learning technique that involves combining multiple weak models to create a strong model that can make more accurate predictions. \n",
        "\n",
        "The idea behind boosting is to iteratively train a series of weak models, such as decision trees, on different subsets of the training data. Each subsequent model is trained to correct the errors of the previous model, with more emphasis placed on the data points that were misclassified. By combining the predictions of all the models, the final prediction can be more accurate and robust than any of the individual models.\n",
        "\n",
        "For example, if we want to predict whether a customer will buy a product based on their age, gender, and income, we can use boosting to train a series of decision tree models on different subsets of the customer data. Each decision tree model might focus on different features of the customer data, such as age or income, and produce slightly different predictions. The subsequent models are trained to correct the errors of the previous models and give more weight to the data points that were misclassified. The final prediction is a weighted combination of the predictions from all the decision tree models, which can improve the accuracy of the model.\n",
        "\n",
        "Boosting is useful for improving the accuracy of models that tend to have high bias or low variance, such as decision trees, by combining the strengths of multiple weak models."
      ],
      "metadata": {
        "id": "WKiD4U1z8eia"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xdqyzjsW7rOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}